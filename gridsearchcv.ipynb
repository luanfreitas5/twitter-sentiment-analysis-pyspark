{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network._multilayer_perceptron import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree._classes import DecisionTreeClassifier\n",
    "from pactools.grid_search import GridSearchCVProgressBar\n",
    "import tqdm\n",
    "import multiprocessing\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['volumeTweets_media', 'volumeTweets_variancia',\n",
       "       'volumeTweets_mediaMovelPonterada', 'volumeTweets_entropia',\n",
       "       'indiceInsonia_mediaMovelPonterada', 'indiceInsonia_entropia',\n",
       "       'pronome1Pessoa_media', 'pronome1Pessoa_variancia',\n",
       "       'pronome1Pessoa_mediaMovelPonterada', 'pronome1Pessoa_entropia',\n",
       "       'pronome2Pessoa_mediaMovelPonterada', 'pronome3Pessoa_variancia',\n",
       "       'pronome3Pessoa_mediaMovelPonterada', 'pronome3Pessoa_entropia',\n",
       "       'valencia_variancia', 'valencia_entropia', 'ativacao_variancia',\n",
       "       'ativacao_entropia', 'termosDepressivos_mediaMovelPonterada',\n",
       "       'grafoSocial_variancia', 'grafoSocial_mediaMovelPonterada',\n",
       "       'grafoSocial_entropia', 'caracteresOrientais_variancia',\n",
       "       'emojis_mediaMovelPonterada', 'curtidas_media', 'curtidas_variancia',\n",
       "       'curtidas_mediaMovelPonterada', 'hashtags_variancia',\n",
       "       'hashtags_mediaMovelPonterada', 'retweets_media', 'retweets_variancia',\n",
       "       'retweets_mediaMovelPonterada', 'mencoes_media', 'mencoes_variancia',\n",
       "       'mencoes_mediaMovelPonterada', 'polaridade_entropia',\n",
       "       'subjetividade_entropia'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "periodo = 'pre-pandemia'\n",
    "df = pd.read_csv(f'datasets/twitterbase_{periodo}.csv', sep=';')\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "zscore = MinMaxScaler()\n",
    "x = df.drop('classe', axis=1).copy().abs()\n",
    "y = df['classe'].copy()\n",
    "#x = zscore.fit_transform(x)\n",
    "\n",
    "k = 37\n",
    "selector = SelectKBest(chi2, k=k)\n",
    "X_new = selector.fit_transform(x, y)\n",
    "\n",
    "# Obtenha os índices dos melhores atributos selecionados\n",
    "indices_melhores_atributos = selector.get_support(indices=True)\n",
    "\n",
    "# Obtenha os nomes dos melhores atributos\n",
    "x.columns[indices_melhores_atributos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['volumeTweets_media', 'volumeTweets_variancia', 'volumeTweets_mediaMovelPonterada', 'volumeTweets_entropia', 'indiceInsonia_mediaMovelPonterada', 'indiceInsonia_entropia', 'pronome1Pessoa_media', 'pronome1Pessoa_variancia', 'pronome1Pessoa_mediaMovelPonterada', 'pronome1Pessoa_entropia', 'pronome2Pessoa_mediaMovelPonterada', 'pronome3Pessoa_variancia', 'pronome3Pessoa_mediaMovelPonterada', 'pronome3Pessoa_entropia', 'valencia_variancia', 'valencia_entropia', 'ativacao_variancia', 'ativacao_entropia', 'termosDepressivos_mediaMovelPonterada', 'grafoSocial_variancia', 'grafoSocial_mediaMovelPonterada', 'grafoSocial_entropia', 'caracteresOrientais_variancia', 'emojis_mediaMovelPonterada', 'curtidas_media', 'curtidas_variancia', 'curtidas_mediaMovelPonterada', 'hashtags_variancia', 'hashtags_mediaMovelPonterada', 'retweets_media', 'retweets_variancia', 'retweets_mediaMovelPonterada', 'mencoes_media', 'mencoes_variancia', 'mencoes_mediaMovelPonterada', 'polaridade_entropia', 'subjetividade_entropia', 'classe']\n"
     ]
    }
   ],
   "source": [
    "atributos = x.columns[indices_melhores_atributos].tolist() + ['classe']\n",
    "print(atributos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos = ['volumeTweets_media', 'volumeTweets_variancia', 'volumeTweets_mediaMovelPonterada', 'volumeTweets_entropia', \n",
    " 'indiceInsonia_mediaMovelPonterada', 'indiceInsonia_entropia', \n",
    " 'pronome1Pessoa_media', 'pronome1Pessoa_variancia', 'pronome1Pessoa_mediaMovelPonterada', 'pronome1Pessoa_entropia', \n",
    " 'pronome2Pessoa_mediaMovelPonterada', \n",
    " 'pronome3Pessoa_variancia', 'pronome3Pessoa_mediaMovelPonterada', 'pronome3Pessoa_entropia', \n",
    " 'valencia_variancia', 'valencia_entropia', \n",
    " 'ativacao_variancia', 'ativacao_entropia', \n",
    " 'termosDepressivos_mediaMovelPonterada', \n",
    " 'grafoSocial_variancia', 'grafoSocial_mediaMovelPonterada', 'grafoSocial_entropia',\n",
    " 'medicamentosAntiDepressivo_variancia',\n",
    " 'caracteresOrientais_variancia', \n",
    " 'emojis_mediaMovelPonterada', \n",
    " 'curtidas_media', 'curtidas_variancia', 'curtidas_mediaMovelPonterada',\n",
    " 'midia_variancia',\n",
    " 'links_entropia',\n",
    " 'hashtags_variancia', 'hashtags_mediaMovelPonterada', \n",
    "                   'retweets_media', 'retweets_variancia', 'retweets_mediaMovelPonterada', \n",
    " 'mencoes_media', 'mencoes_variancia', 'mencoes_mediaMovelPonterada', \n",
    " 'polaridade_entropia', \n",
    " 'subjetividade_entropia', 'classe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "zscore = StandardScaler()\n",
    "\n",
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "df = pd.read_csv(f'datasets/twitterbase_{periodo}.csv', sep=';', usecols=atributos)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['classe'] = label_encoder.fit_transform(df['classe']).astype('float64')\n",
    "\n",
    "x = df.drop('classe', axis=1).copy()\n",
    "y = df['classe'].copy()\n",
    "x = zscore.fit_transform(x)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=validation_ratio, random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=test_ratio, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hiperparâmetros: {'criterion': 'gini'}\n",
      "F1-score do Modelo: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Definindo o classificador (DecisionTreeClassifier)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "\n",
    "# Definindo os hiperparâmetros que queremos otimizar\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=cv, scoring='f1', verbose=10)\n",
    "\n",
    "# Realizando a busca pelos melhores hiperparâmetros\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores Hiperparâmetros:\", best_params)\n",
    "\n",
    "# Obtendo o modelo com os melhores hiperparâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazendo previsões no conjunto de validacao\n",
    "y_pred = best_model.predict(x_val)\n",
    "\n",
    "# Calculando o f1-score do modelo no conjunto de validacao\n",
    "f1score = f1_score(y_val, y_pred).round(2)\n",
    "print(\"F1-score do Modelo:\", f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hiperparâmetros: {'C': 0.1, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "F1-score do Modelo: 0.73\n"
     ]
    }
   ],
   "source": [
    "# Definindo o classificador (LinearSVC)\n",
    "svc_classifier = LinearSVC(random_state=42, verbose=10, max_iter=100)\n",
    "\n",
    "# Definindo os hiperparâmetros que queremos otimizar\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'loss': ['squared_hinge', 'hinge']\n",
    "}\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svc_classifier, param_grid=param_grid, cv=cv, scoring='f1', verbose=10)\n",
    "\n",
    "# Realizando a busca pelos melhores hiperparâmetros\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores Hiperparâmetros:\", best_params)\n",
    "\n",
    "# Obtendo o modelo com os melhores hiperparâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazendo previsões no conjunto de validacao\n",
    "y_pred = best_model.predict(x_val)\n",
    "\n",
    "# Calculando o f1-score do modelo no conjunto de validacao\n",
    "f1score = f1_score(y_val, y_pred).round(2)\n",
    "print(\"F1-score do Modelo:\", f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hiperparâmetros: {'activation': 'relu', 'hidden_layer_sizes': (50, 50)}\n",
      "F1-score do Modelo: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Definindo o classificador (MLPClassifier)\n",
    "mlp_classifier = MLPClassifier(random_state=42, max_iter=100, n_iter_no_change=20, batch_size=128, early_stopping=True, solver='adam', shuffle=True)\n",
    "\n",
    "# Definindo os hiperparâmetros que queremos otimizar\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (10,30,10)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "}\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=mlp_classifier, param_grid=param_grid, cv=cv, scoring='f1', verbose=10)\n",
    "\n",
    "# Realizando a busca pelos melhores hiperparâmetros\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores Hiperparâmetros:\", best_params)\n",
    "\n",
    "# Obtendo o modelo com os melhores hiperparâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazendo previsões no conjunto de validacao\n",
    "y_pred = best_model.predict(x_val)\n",
    "\n",
    "# Calculando o f1-score do modelo no conjunto de validacao\n",
    "f1score = f1_score(y_val, y_pred).round(2)\n",
    "print(\"F1-score do Modelo:\", f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hiperparâmetros: {'criterion': 'gini', 'n_estimators': 50}\n",
      "F1-score do Modelo: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done   4 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  11 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  27 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  43 out of  50 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Definindo o classificador (RandomForestClassifier)\n",
    "rf_classifier = RandomForestClassifier(random_state=42, verbose=10, n_jobs=n_jobs, max_depth=10)\n",
    "\n",
    "# Definindo os hiperparâmetros que queremos otimizar\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=cv, scoring='f1', verbose=10)\n",
    "\n",
    "# Realizando a busca pelos melhores hiperparâmetros\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores Hiperparâmetros:\", best_params)\n",
    "\n",
    "# Obtendo o modelo com os melhores hiperparâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazendo previsões no conjunto de validacao\n",
    "y_pred = best_model.predict(x_val)\n",
    "\n",
    "# Calculando o f1-score do modelo no conjunto de validacao\n",
    "f1score = f1_score(y_val, y_pred).round(2)\n",
    "print(\"F1-score do Modelo:\", f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['volumeTweets_media', 'volumeTweets_variancia',\n",
       "       'volumeTweets_mediaMovelPonterada', 'volumeTweets_entropia',\n",
       "       'indiceInsonia_variancia', 'indiceInsonia_mediaMovelPonterada',\n",
       "       'indiceInsonia_entropia', 'pronome1Pessoa_media',\n",
       "       'pronome1Pessoa_variancia', 'pronome1Pessoa_mediaMovelPonterada',\n",
       "       'pronome1Pessoa_entropia', 'pronome2Pessoa_mediaMovelPonterada',\n",
       "       'pronome3Pessoa_media', 'pronome3Pessoa_variancia',\n",
       "       'pronome3Pessoa_mediaMovelPonterada', 'pronome3Pessoa_entropia',\n",
       "       'valencia_mediaMovelPonterada', 'valencia_entropia',\n",
       "       'ativacao_mediaMovelPonterada', 'ativacao_entropia',\n",
       "       'termosDepressivos_variancia', 'grafoSocial_variancia',\n",
       "       'grafoSocial_mediaMovelPonterada', 'grafoSocial_entropia',\n",
       "       'caracteresOrientais_variancia', 'emojis_variancia', 'midia_variancia',\n",
       "       'midia_mediaMovelPonterada', 'curtidas_media', 'curtidas_variancia',\n",
       "       'curtidas_mediaMovelPonterada', 'hashtags_variancia',\n",
       "       'hashtags_mediaMovelPonterada', 'retweets_variancia',\n",
       "       'mencoes_variancia', 'mencoes_mediaMovelPonterada',\n",
       "       'polaridade_entropia'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "periodo = 'pandemia'\n",
    "df = pd.read_csv(f'datasets/twitterbase_{periodo}.csv', sep=';')\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "zscore = MinMaxScaler()\n",
    "x = df.drop('classe', axis=1).copy().abs()\n",
    "y = df['classe'].copy()\n",
    "#x = zscore.fit_transform(x)\n",
    "\n",
    "k = 37\n",
    "selector = SelectKBest(chi2, k=k)\n",
    "X_new = selector.fit_transform(x, y)\n",
    "\n",
    "# Obtenha os índices dos melhores atributos selecionados\n",
    "indices_melhores_atributos = selector.get_support(indices=True)\n",
    "\n",
    "# Obtenha os nomes dos melhores atributos\n",
    "x.columns[indices_melhores_atributos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['volumeTweets_media', 'volumeTweets_variancia', 'volumeTweets_mediaMovelPonterada', 'volumeTweets_entropia', 'indiceInsonia_variancia', 'indiceInsonia_mediaMovelPonterada', 'indiceInsonia_entropia', 'pronome1Pessoa_media', 'pronome1Pessoa_variancia', 'pronome1Pessoa_mediaMovelPonterada', 'pronome1Pessoa_entropia', 'pronome2Pessoa_mediaMovelPonterada', 'pronome3Pessoa_media', 'pronome3Pessoa_variancia', 'pronome3Pessoa_mediaMovelPonterada', 'pronome3Pessoa_entropia', 'valencia_mediaMovelPonterada', 'valencia_entropia', 'ativacao_mediaMovelPonterada', 'ativacao_entropia', 'termosDepressivos_variancia', 'grafoSocial_variancia', 'grafoSocial_mediaMovelPonterada', 'grafoSocial_entropia', 'caracteresOrientais_variancia', 'emojis_variancia', 'midia_variancia', 'midia_mediaMovelPonterada', 'curtidas_media', 'curtidas_variancia', 'curtidas_mediaMovelPonterada', 'hashtags_variancia', 'hashtags_mediaMovelPonterada', 'retweets_variancia', 'mencoes_variancia', 'mencoes_mediaMovelPonterada', 'polaridade_entropia', 'classe']\n"
     ]
    }
   ],
   "source": [
    "atributos = x.columns[indices_melhores_atributos].tolist() + ['classe']\n",
    "print(atributos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos = ['volumeTweets_media', 'volumeTweets_variancia', 'volumeTweets_mediaMovelPonterada', 'volumeTweets_entropia', \n",
    "'indiceInsonia_variancia', 'indiceInsonia_mediaMovelPonterada', 'indiceInsonia_entropia', \n",
    "'pronome1Pessoa_media', 'pronome1Pessoa_variancia', 'pronome1Pessoa_mediaMovelPonterada', 'pronome1Pessoa_entropia', \n",
    "'pronome2Pessoa_mediaMovelPonterada', \n",
    "'pronome3Pessoa_media', 'pronome3Pessoa_variancia', 'pronome3Pessoa_mediaMovelPonterada', 'pronome3Pessoa_entropia', \n",
    "'valencia_mediaMovelPonterada', 'valencia_entropia', \n",
    "'ativacao_mediaMovelPonterada', 'ativacao_entropia', \n",
    "'termosDepressivos_variancia', \n",
    "'grafoSocial_variancia', 'grafoSocial_mediaMovelPonterada', 'grafoSocial_entropia',\n",
    "'medicamentosAntiDepressivo_mediaMovelPonterada',\n",
    "'caracteresOrientais_variancia', \n",
    "'emojis_variancia', \n",
    "'midia_variancia', 'midia_mediaMovelPonterada', \n",
    "'curtidas_media', 'curtidas_variancia', 'curtidas_mediaMovelPonterada',\n",
    "'links_mediaMovelPonterada',\n",
    "'hashtags_variancia', 'hashtags_mediaMovelPonterada', \n",
    "'retweets_variancia', \n",
    "'mencoes_variancia', 'mencoes_mediaMovelPonterada', \n",
    "'polaridade_entropia', \n",
    "'subjetividade_entropia', 'classe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "zscore = StandardScaler()\n",
    "\n",
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "df = pd.read_csv(f'datasets/twitterbase_{periodo}.csv', sep=';')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['classe'] = label_encoder.fit_transform(df['classe']).astype('float64')\n",
    "\n",
    "x = df.drop('classe', axis=1).copy()\n",
    "y = df['classe'].copy()\n",
    "x = zscore.fit_transform(x)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=validation_ratio, random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=test_ratio, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hiperparâmetros: {'criterion': 'entropy'}\n",
      "F1-score do Modelo: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Definindo o classificador (DecisionTreeClassifier)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "\n",
    "# Definindo os hiperparâmetros que queremos otimizar\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=cv, scoring='f1', verbose=10)\n",
    "\n",
    "# Realizando a busca pelos melhores hiperparâmetros\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores Hiperparâmetros:\", best_params)\n",
    "\n",
    "# Obtendo o modelo com os melhores hiperparâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazendo previsões no conjunto de validacao\n",
    "y_pred = best_model.predict(x_val)\n",
    "\n",
    "# Calculando o f1-score do modelo no conjunto de validacao\n",
    "f1score = f1_score(y_val, y_pred).round(2)\n",
    "print(\"F1-score do Modelo:\", f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hiperparâmetros: {'C': 1.0, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "F1-score do Modelo: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Definindo o classificador (LinearSVC)\n",
    "svc_classifier = LinearSVC(random_state=42, verbose=10, max_iter=100)\n",
    "\n",
    "# Definindo os hiperparâmetros que queremos otimizar\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'loss': ['squared_hinge', 'hinge']\n",
    "}\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svc_classifier, param_grid=param_grid, cv=cv, scoring='f1', verbose=10)\n",
    "\n",
    "# Realizando a busca pelos melhores hiperparâmetros\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores Hiperparâmetros:\", best_params)\n",
    "\n",
    "# Obtendo o modelo com os melhores hiperparâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazendo previsões no conjunto de validacao\n",
    "y_pred = best_model.predict(x_val)\n",
    "\n",
    "# Calculando o f1-score do modelo no conjunto de validacao\n",
    "f1score = f1_score(y_val, y_pred).round(2)\n",
    "print(\"F1-score do Modelo:\", f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hiperparâmetros: {'activation': 'relu', 'hidden_layer_sizes': (50, 50)}\n",
      "F1-score do Modelo: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Definindo o classificador (MLPClassifier)\n",
    "mlp_classifier = MLPClassifier(random_state=42, max_iter=100, n_iter_no_change=20, batch_size=128, early_stopping=True, solver='adam', shuffle=True)\n",
    "\n",
    "# Definindo os hiperparâmetros que queremos otimizar\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (10,30,10)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "}\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=mlp_classifier, param_grid=param_grid, cv=cv, scoring='f1', verbose=10)\n",
    "\n",
    "# Realizando a busca pelos melhores hiperparâmetros\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores Hiperparâmetros:\", best_params)\n",
    "\n",
    "# Obtendo o modelo com os melhores hiperparâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazendo previsões no conjunto de validacao\n",
    "y_pred = best_model.predict(x_val)\n",
    "\n",
    "# Calculando o f1-score do modelo no conjunto de validacao\n",
    "f1score = f1_score(y_val, y_pred).round(2)\n",
    "print(\"F1-score do Modelo:\", f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hiperparâmetros: {'criterion': 'gini', 'n_estimators': 50}\n",
      "F1-score do Modelo: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done   4 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  11 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  27 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  43 out of  50 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Definindo o classificador (RandomForestClassifier)\n",
    "rf_classifier = RandomForestClassifier(random_state=42, verbose=10, n_jobs=n_jobs, max_depth=10)\n",
    "\n",
    "# Definindo os hiperparâmetros que queremos otimizar\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=cv, scoring='f1', verbose=10)\n",
    "\n",
    "# Realizando a busca pelos melhores hiperparâmetros\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores Hiperparâmetros:\", best_params)\n",
    "\n",
    "# Obtendo o modelo com os melhores hiperparâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazendo previsões no conjunto de validacao\n",
    "y_pred = best_model.predict(x_val)\n",
    "\n",
    "# Calculando o f1-score do modelo no conjunto de validacao\n",
    "f1score = f1_score(y_val, y_pred).round(2)\n",
    "print(\"F1-score do Modelo:\", f1score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
