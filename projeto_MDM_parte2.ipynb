{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35e0565-282b-4e57-914f-1ddd9875926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import entropy\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75613e3-d6cd-4f69-bb69-8a573785628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def abrirBaseSerie(caracteristica, diretorioCaracteristicas) -> dict:\n",
    "    arquivoPickle = f'{diretorioCaracteristicas}{caracteristica}_serieTemporal.pickle'\n",
    "    \n",
    "    try:\n",
    "        with open(arquivoPickle, 'rb') as f:\n",
    "            return pickle.load(f, fix_imports=False)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Arquivo Pickle com Erro \\n{e}\")\n",
    "        return {}\n",
    "\n",
    "    \n",
    "def parseMediaPonderada(series) -> float:\n",
    "    try:\n",
    "        series_media = series.mean(skipna=True)\n",
    "        return series_media\n",
    "    except Exception:\n",
    "        return series\n",
    "\n",
    "\n",
    "def parseVariancia(series) -> float:\n",
    "    try:\n",
    "        series_var = series.var(ddof=0, skipna=True)\n",
    "        return series_var\n",
    "    except Exception:\n",
    "        series_media = parseMediaPonderada(series)\n",
    "        return series_media\n",
    "\n",
    "    \n",
    "def parseMediaMovelPonderada(series) -> float:\n",
    "    try:\n",
    "        if len(series) > 7:\n",
    "            convolucao = np.convolve(series, np.ones(7), 'valid')\n",
    "            mediaMovel = sum(convolucao) / (len(series) - 7)\n",
    "            return mediaMovel\n",
    "        else:\n",
    "            series_media = parseMediaPonderada(series)\n",
    "            return series_media\n",
    "        \n",
    "    except Exception:\n",
    "        series_media = parseMediaPonderada(series)\n",
    "        return series_media\n",
    "\n",
    "\n",
    "def parseEntropia(series) -> float:\n",
    "    try:\n",
    "        series_entropy = entropy(series.value_counts())\n",
    "        return series_entropy\n",
    "    except Exception:\n",
    "        series_media = parseMediaPonderada(series)\n",
    "        return series_media\n",
    "\n",
    "    \n",
    "atributo_volume_tweets, atributo_volume_tweets_noite = 'volumeTweets', 'volumeTweetsNoite'\n",
    "atributo_indice_insonia, atributo_pronome_1_pessoa = 'indiceInsonia', 'pronome1Pessoa'\n",
    "atributo_pronome_2_pessoa, atributo_pronome_3_pessoa = 'pronome2Pessoa', 'pronome3Pessoa'\n",
    "atributo_valencia, atributo_ativacao, atributo_termos_depressivos = 'valencia', 'ativacao', 'termosDepressivos'\n",
    "atributo_medicamentos_anti_depressivo, atributo_grafo_social = 'medicamentosAntiDepressivo', 'grafoSocial'\n",
    "atributo_caracteres_orientais, atributo_emojis = 'caracteresOrientais', 'emojis'\n",
    "atributo_links, atributo_midia, atributo_curtidas = 'links', 'midia', 'curtidas'\n",
    "atributo_hashtags, atributo_retweets = 'hashtags', 'retweets'\n",
    "atributo_mencoes, atributo_polaridade, atributo_subjetividade = 'mencoes', 'polaridade', 'subjetividade'\n",
    "\n",
    "listaAtributos = [atributo_volume_tweets,\n",
    "                  atributo_indice_insonia, atributo_pronome_1_pessoa,\n",
    "                  atributo_pronome_2_pessoa, atributo_pronome_3_pessoa,\n",
    "                  atributo_valencia, atributo_ativacao,\n",
    "                  atributo_termos_depressivos, atributo_grafo_social,\n",
    "                  atributo_medicamentos_anti_depressivo,\n",
    "                  atributo_caracteres_orientais, atributo_emojis,\n",
    "                  atributo_links, atributo_midia, atributo_curtidas,\n",
    "                  atributo_hashtags, atributo_retweets, atributo_mencoes,\n",
    "                  atributo_polaridade, atributo_subjetividade]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bcd6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sumarizando atributos pandemia depressao volumeTweets: 100%|\u001b[32m██████████\u001b[0m| 20/20 [06:21<00:00, 19.08s/it]             \n"
     ]
    }
   ],
   "source": [
    "\n",
    "periodo = 'pandemia'\n",
    "grupo = 'depressao'\n",
    "diretorioCaracteristicas = f'datasets/{periodo}/caracteristicas/{grupo}/'\n",
    "\n",
    "arquivoVetoresCaracteristicas = f'{diretorioCaracteristicas}{periodo}_{grupo}_vetoresCaracteristicas.csv'\n",
    "\n",
    "listaCaracteristicas = [[f'{i}_media', f'{i}_variancia', f'{i}_mediaMovelPonterada', f'{i}_entropia'] for i in listaAtributos]\n",
    "listaCaracteristicas = [i for lista in listaCaracteristicas for i in lista]\n",
    "\n",
    "listaCandidatos = list(set(abrirBaseSerie(atributo_volume_tweets, diretorioCaracteristicas)))\n",
    "listaCandidatos.sort()\n",
    "\n",
    "if os.path.isfile(arquivoVetoresCaracteristicas):\n",
    "    df = pd.read_csv(arquivoVetoresCaracteristicas, sep=';', index_col=0, encoding='utf-8')\n",
    "    listaCandidatosProcessados = set(df.index)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=listaCaracteristicas, index=listaCandidatos, data=0.0)\n",
    "    listaCandidatosProcessados = set()\n",
    "\n",
    "listaSeriesTemporais = [fnames for fnames in os.listdir(diretorioCaracteristicas) if '_serieTemporal' in fnames]\n",
    "\n",
    "listaNovosCandidatos = list(set(listaCandidatos) - set(listaCandidatosProcessados))\n",
    "listaNovosCandidatos.sort()\n",
    "\n",
    "del listaCandidatos\n",
    "del listaCandidatosProcessados\n",
    "\n",
    "if listaNovosCandidatos:\n",
    "    \n",
    "    for candidato in listaNovosCandidatos:\n",
    "        df.loc[candidato] = 0.0\n",
    "        \n",
    "    df.sort_index(axis=0, inplace=True)\n",
    "    \n",
    "    pbar = tqdm.tqdm(listaSeriesTemporais, colour=\"green\")\n",
    "        \n",
    "    # carregando series temporais de cada caracteristicas\n",
    "    for serie in pbar:\n",
    "        seriesTemporais = {}\n",
    "        atributo = serie.rsplit('_', 1)[0]\n",
    "        pbar.set_description(f\"Sumarizando atributos {periodo} {grupo} {atributo}\")\n",
    "        \n",
    "        with open(diretorioCaracteristicas + serie, 'rb') as f:\n",
    "            seriesTemporais[atributo] = pickle.load(f)\n",
    "        \n",
    "        for candidato in listaNovosCandidatos:\n",
    "            \n",
    "            for atributo in seriesTemporais.keys():\n",
    "                df.loc[candidato][f'{atributo}_media'] = parseMediaPonderada(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_variancia'] = parseVariancia(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_mediaMovelPonterada'] = parseMediaMovelPonderada(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_entropia'] = parseEntropia(seriesTemporais[atributo][candidato])\n",
    "                    \n",
    "    if not os.path.exists(arquivoVetoresCaracteristicas):\n",
    "        df.to_csv(arquivoVetoresCaracteristicas, sep=';', encoding='utf-8')\n",
    "    #else:\n",
    "        #df.to_csv(arquivoVetoresCaracteristicas, sep=';', header=False, mode='a', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a592d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sumarizando atributos pandemia controle volumeTweets: 100%|\u001b[32m██████████\u001b[0m| 20/20 [12:06<00:00, 36.30s/it]             \n"
     ]
    }
   ],
   "source": [
    "\n",
    "periodo = 'pandemia'\n",
    "grupo = 'controle'\n",
    "diretorioCaracteristicas = f'datasets/{periodo}/caracteristicas/{grupo}/'\n",
    "\n",
    "arquivoVetoresCaracteristicas = f'{diretorioCaracteristicas}{periodo}_{grupo}_vetoresCaracteristicas.csv'\n",
    "\n",
    "listaCaracteristicas = [[f'{i}_media', f'{i}_variancia', f'{i}_mediaMovelPonterada', f'{i}_entropia'] for i in listaAtributos]\n",
    "listaCaracteristicas = [i for lista in listaCaracteristicas for i in lista]\n",
    "\n",
    "listaCandidatos = list(set(abrirBaseSerie(atributo_volume_tweets, diretorioCaracteristicas)))\n",
    "listaCandidatos.sort()\n",
    "\n",
    "if os.path.isfile(arquivoVetoresCaracteristicas):\n",
    "    df = pd.read_csv(arquivoVetoresCaracteristicas, sep=';', index_col=0, encoding='utf-8')\n",
    "    listaCandidatosProcessados = set(df.index)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=listaCaracteristicas, index=listaCandidatos, data=0.0)\n",
    "    listaCandidatosProcessados = set()\n",
    "\n",
    "listaSeriesTemporais = [fnames for fnames in os.listdir(diretorioCaracteristicas) if '_serieTemporal' in fnames]\n",
    "\n",
    "listaNovosCandidatos = list(set(listaCandidatos) - set(listaCandidatosProcessados))\n",
    "listaNovosCandidatos.sort()\n",
    "\n",
    "del listaCandidatos\n",
    "del listaCandidatosProcessados\n",
    "\n",
    "if listaNovosCandidatos:\n",
    "    \n",
    "    for candidato in listaNovosCandidatos:\n",
    "        df.loc[candidato] = 0.0\n",
    "        \n",
    "    df.sort_index(axis=0, inplace=True)\n",
    "    \n",
    "    pbar = tqdm.tqdm(listaSeriesTemporais, colour=\"green\")\n",
    "        \n",
    "    # carregando series temporais de cada caracteristicas\n",
    "    for serie in pbar:\n",
    "        seriesTemporais = {}\n",
    "        atributo = serie.rsplit('_', 1)[0]\n",
    "        pbar.set_description(f\"Sumarizando atributos {periodo} {grupo} {atributo}\")\n",
    "        \n",
    "        with open(diretorioCaracteristicas + serie, 'rb') as f:\n",
    "            seriesTemporais[atributo] = pickle.load(f)\n",
    "        \n",
    "        for candidato in listaNovosCandidatos:\n",
    "            \n",
    "            for atributo in seriesTemporais.keys():\n",
    "                df.loc[candidato][f'{atributo}_media'] = parseMediaPonderada(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_variancia'] = parseVariancia(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_mediaMovelPonterada'] = parseMediaMovelPonderada(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_entropia'] = parseEntropia(seriesTemporais[atributo][candidato])\n",
    "                    \n",
    "    if not os.path.exists(arquivoVetoresCaracteristicas):\n",
    "        df.to_csv(arquivoVetoresCaracteristicas, sep=';', encoding='utf-8')\n",
    "    #else:\n",
    "        #df.to_csv(arquivoVetoresCaracteristicas, sep=';', header=False, mode='a', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526b2711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sumarizando atributos pre-pandemia depressao volumeTweets: 100%|\u001b[32m██████████\u001b[0m| 20/20 [06:22<00:00, 19.11s/it]             \n"
     ]
    }
   ],
   "source": [
    "\n",
    "periodo = 'pre-pandemia'\n",
    "grupo = 'depressao'\n",
    "diretorioCaracteristicas = f'datasets/{periodo}/caracteristicas/{grupo}/'\n",
    "\n",
    "arquivoVetoresCaracteristicas = f'{diretorioCaracteristicas}{periodo}_{grupo}_vetoresCaracteristicas.csv'\n",
    "\n",
    "listaCaracteristicas = [[f'{i}_media', f'{i}_variancia', f'{i}_mediaMovelPonterada', f'{i}_entropia'] for i in listaAtributos]\n",
    "listaCaracteristicas = [i for lista in listaCaracteristicas for i in lista]\n",
    "\n",
    "listaCandidatos = list(set(abrirBaseSerie(atributo_volume_tweets, diretorioCaracteristicas)))\n",
    "listaCandidatos.sort()\n",
    "\n",
    "if os.path.isfile(arquivoVetoresCaracteristicas):\n",
    "    df = pd.read_csv(arquivoVetoresCaracteristicas, sep=';', index_col=0, encoding='utf-8')\n",
    "    listaCandidatosProcessados = set(df.index)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=listaCaracteristicas, index=listaCandidatos, data=0.0)\n",
    "    listaCandidatosProcessados = set()\n",
    "\n",
    "listaSeriesTemporais = [fnames for fnames in os.listdir(diretorioCaracteristicas) if '_serieTemporal' in fnames]\n",
    "\n",
    "listaNovosCandidatos = list(set(listaCandidatos) - set(listaCandidatosProcessados))\n",
    "listaNovosCandidatos.sort()\n",
    "\n",
    "del listaCandidatos\n",
    "del listaCandidatosProcessados\n",
    "\n",
    "if listaNovosCandidatos:\n",
    "    \n",
    "    for candidato in listaNovosCandidatos:\n",
    "        df.loc[candidato] = 0.0\n",
    "        \n",
    "    df.sort_index(axis=0, inplace=True)\n",
    "    \n",
    "    pbar = tqdm.tqdm(listaSeriesTemporais, colour=\"green\")\n",
    "        \n",
    "    # carregando series temporais de cada caracteristicas\n",
    "    for serie in pbar:\n",
    "        seriesTemporais = {}\n",
    "        atributo = serie.rsplit('_', 1)[0]\n",
    "        pbar.set_description(f\"Sumarizando atributos {periodo} {grupo} {atributo}\")\n",
    "        \n",
    "        with open(diretorioCaracteristicas + serie, 'rb') as f:\n",
    "            seriesTemporais[atributo] = pickle.load(f)\n",
    "        \n",
    "        for candidato in listaNovosCandidatos:\n",
    "            \n",
    "            for atributo in seriesTemporais.keys():\n",
    "                df.loc[candidato][f'{atributo}_media'] = parseMediaPonderada(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_variancia'] = parseVariancia(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_mediaMovelPonterada'] = parseMediaMovelPonderada(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_entropia'] = parseEntropia(seriesTemporais[atributo][candidato])\n",
    "                    \n",
    "    if not os.path.exists(arquivoVetoresCaracteristicas):\n",
    "        df.to_csv(arquivoVetoresCaracteristicas, sep=';', encoding='utf-8')\n",
    "    #else:\n",
    "        #df.to_csv(arquivoVetoresCaracteristicas, sep=';', header=False, mode='a', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a8ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sumarizando atributos pre-pandemia controle volumeTweets: 100%|\u001b[32m██████████\u001b[0m| 20/20 [05:37<00:00, 16.87s/it]             \n"
     ]
    }
   ],
   "source": [
    "\n",
    "periodo = 'pre-pandemia'\n",
    "grupo = 'controle'\n",
    "diretorioCaracteristicas = f'datasets/{periodo}/caracteristicas/{grupo}/'\n",
    "\n",
    "arquivoVetoresCaracteristicas = f'{diretorioCaracteristicas}{periodo}_{grupo}_vetoresCaracteristicas.csv'\n",
    "\n",
    "listaCaracteristicas = [[f'{i}_media', f'{i}_variancia', f'{i}_mediaMovelPonterada', f'{i}_entropia'] for i in listaAtributos]\n",
    "listaCaracteristicas = [i for lista in listaCaracteristicas for i in lista]\n",
    "\n",
    "listaCandidatos = list(set(abrirBaseSerie(atributo_volume_tweets, diretorioCaracteristicas)))\n",
    "listaCandidatos.sort()\n",
    "\n",
    "if os.path.isfile(arquivoVetoresCaracteristicas):\n",
    "    df = pd.read_csv(arquivoVetoresCaracteristicas, sep=';', index_col=0, encoding='utf-8')\n",
    "    listaCandidatosProcessados = set(df.index)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=listaCaracteristicas, index=listaCandidatos, data=0.0)\n",
    "    listaCandidatosProcessados = set()\n",
    "\n",
    "listaSeriesTemporais = [fnames for fnames in os.listdir(diretorioCaracteristicas) if '_serieTemporal' in fnames]\n",
    "\n",
    "listaNovosCandidatos = list(set(listaCandidatos) - set(listaCandidatosProcessados))\n",
    "listaNovosCandidatos.sort()\n",
    "\n",
    "del listaCandidatos\n",
    "del listaCandidatosProcessados\n",
    "\n",
    "if listaNovosCandidatos:\n",
    "    \n",
    "    for candidato in listaNovosCandidatos:\n",
    "        df.loc[candidato] = 0.0\n",
    "        \n",
    "    df.sort_index(axis=0, inplace=True)\n",
    "    \n",
    "    pbar = tqdm.tqdm(listaSeriesTemporais, colour=\"green\")\n",
    "        \n",
    "    # carregando series temporais de cada caracteristicas\n",
    "    for serie in pbar:\n",
    "        seriesTemporais = {}\n",
    "        atributo = serie.rsplit('_', 1)[0]\n",
    "        pbar.set_description(f\"Sumarizando atributos {periodo} {grupo} {atributo}\")\n",
    "        \n",
    "        with open(diretorioCaracteristicas + serie, 'rb') as f:\n",
    "            seriesTemporais[atributo] = pickle.load(f)\n",
    "        \n",
    "        for candidato in listaNovosCandidatos:\n",
    "            \n",
    "            for atributo in seriesTemporais.keys():\n",
    "                df.loc[candidato][f'{atributo}_media'] = parseMediaPonderada(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_variancia'] = parseVariancia(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_mediaMovelPonterada'] = parseMediaMovelPonderada(seriesTemporais[atributo][candidato])\n",
    "                df.loc[candidato][f'{atributo}_entropia'] = parseEntropia(seriesTemporais[atributo][candidato])\n",
    "                    \n",
    "    if not os.path.exists(arquivoVetoresCaracteristicas):\n",
    "        df.to_csv(arquivoVetoresCaracteristicas, sep=';', encoding='utf-8')\n",
    "    #else:\n",
    "        #df.to_csv(arquivoVetoresCaracteristicas, sep=';', header=False, mode='a', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db498c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def normalizarBaseDados(df, periodo):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): _description_\n",
    "        periodo (str): _description_\n",
    "    \"\"\"\n",
    "    zscore = StandardScaler()\n",
    "    data_zscore = zscore.fit_transform(df.iloc[:,:-1])\n",
    "    df_zscore = pd.DataFrame(data=data_zscore)\n",
    "    df_zscore['classe'] = df['classe']\n",
    "    df_zscore.columns = df.columns\n",
    "    \n",
    "    df_zscore.to_csv(f'datasets/twitterbase_{periodo}_zscore.csv', sep=';', index=False)\n",
    "    print(f'datasets/twitterbase_{periodo}_zscore.csv criada')\n",
    "    \n",
    "    df_minmax = MinMaxScaler()\n",
    "    data_minmax = df_minmax.fit_transform(df.iloc[:,:-1])\n",
    "    df_minmax = pd.DataFrame(data=data_minmax)\n",
    "    df_minmax['classe'] = df['classe']\n",
    "    df_minmax.columns = df.columns\n",
    "    \n",
    "    df_minmax.to_csv(f'datasets/twitterbase_{periodo}_minmax.csv', sep=';', index=False)\n",
    "    print(f'datasets/twitterbase_{periodo}_minmax.csv criada')\n",
    "\n",
    "def construirBaseDadosTwitter(periodo, diretorioDepressao, diretorioControle):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        periodo (str): _description_\n",
    "        diretorioDepressao (str): _description_\n",
    "        diretorioControle (str): _description_\n",
    "    \"\"\"\n",
    "    dfDepressao = pd.read_csv(f'{diretorioDepressao}{periodo}_depressao_vetoresCaracteristicas.csv', sep=';', index_col=0)\n",
    "    dfDepressao['classe'] = ['depressao'] * len(dfDepressao)\n",
    "    \n",
    "    dfControle = pd.read_csv(f'{diretorioControle}{periodo}_controle_vetoresCaracteristicas.csv', sep=';', index_col=0)\n",
    "    dfControle['classe'] = ['controle'] * len(dfControle)\n",
    "    \n",
    "    df = pd.concat([dfDepressao, dfControle])\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.to_csv(f'datasets/twitterbase_{periodo}.csv', sep=';', index=False)\n",
    "    print(f'Dataset twitterbase_{periodo}.csv criada')\n",
    "    \n",
    "    #normalizarBaseDados(df, periodo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc988f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset twitterbase_pre-pandemia.csv criada\n"
     ]
    }
   ],
   "source": [
    "\n",
    "periodo = 'pre-pandemia'\n",
    "\n",
    "diretorioDepressao = f'datasets/{periodo}/caracteristicas/depressao/'\n",
    "diretorioControle = f'datasets/{periodo}/caracteristicas/controle/'\n",
    "\n",
    "construirBaseDadosTwitter(periodo, diretorioDepressao, diretorioControle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2fe19bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset twitterbase_pandemia.csv criada\n"
     ]
    }
   ],
   "source": [
    "periodo = 'pandemia'\n",
    "\n",
    "diretorioDepressao = f'datasets/{periodo}/caracteristicas/depressao/'\n",
    "diretorioControle = f'datasets/{periodo}/caracteristicas/controle/'\n",
    "\n",
    "construirBaseDadosTwitter(periodo, diretorioDepressao, diretorioControle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
